{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ordinal and Monotonic Classification\n",
    "\n",
    "Alumno: Ignacio Sánchez Herrera"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejercicio 1\n",
    "### Multiple Model for Ordinal Classification\n",
    "\n",
    "En este apartado compararemos tres modelos distintos empleando\n",
    "el enfoque Múltiple Ordinal y sin emplearlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from sklearn.base import ClassifierMixin, BaseEstimator, clone\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clase para transformar cualquier modelo en un clasificador ordinal.\n",
    "class MultipleModelOrdinalClassifier(BaseEstimator,ClassifierMixin):\n",
    "    # I inherit from these classes so that the class is compatible with the sklearn API.\n",
    "    def __init__(self, classifier):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        :classifier: classifier from which to build the model\n",
    "        \"\"\"\n",
    "        self.classifier = classifier\n",
    "        self.fitted_classifiers = []\n",
    "\n",
    "    def _fit(self,X,y):\n",
    "        #We keep the labels and put them in order\n",
    "        self.labels = np.sort(y.unique())\n",
    "\n",
    "        #Iterate through all the labels except the last one\n",
    "        for i,label in enumerate(self.labels[:-1]):\n",
    "            smaller_labels = self.labels[:i+1] #We make the set of labels <= than the current one\n",
    "            greater_labels = self.labels[i+1:] #Makes the set of labels > than the current one\n",
    "\n",
    "            #We build two dictionaries to replace the label values in the dataset.\n",
    "            smaller_replacements = { label:0 for label in smaller_labels} \n",
    "            greater_replacements = { label:1 for label in greater_labels} \n",
    "            smaller_replacements.update(greater_replacements)\n",
    "\n",
    "            y_i = y.replace(smaller_replacements)\n",
    "\n",
    "            #We create a classifier and fit it to the dataset on the replaced labels.\n",
    "            classifier = clone(self.classifier)\n",
    "            classifier.fit(X,y_i)\n",
    "            #We put the classifier in the fitter_classifiers\n",
    "            self.fitted_classifiers.append(classifier)\n",
    "    \n",
    "    def _predict(self,X):\n",
    "        predictions_greater = []\n",
    "        \n",
    "        #For each classifier we draw predictions in the form of probabilities.\n",
    "        for cl in self.fitted_classifiers:\n",
    "            prediction = cl.predict_proba(X)[:,1] #We will stick with the second probability, i.e. with P(target > Vi)\n",
    "            predictions_greater.append(prediction)\n",
    "\n",
    "        #We compute the probabilities of the first and last class\n",
    "        primera = 1 - predictions_greater[0] #first\n",
    "        ultima = predictions_greater[-1] # last\n",
    "        probabilidades_clase = [primera]\n",
    "        #We calculate the remaining probabilities \n",
    "        for i,pred in enumerate(predictions_greater):\n",
    "            if i != 0 and i != len(predictions_greater): #We avoid going through first and last\n",
    "                prob_i = predictions_greater[i-1]*(1-predictions_greater[i])\n",
    "                probabilidades_clase.append(prob_i)\n",
    "\n",
    "        #We insert the last probability so that they are in order in the array.\n",
    "        probabilidades_clase.append(ultima)\n",
    "\n",
    "        #We take the index of the probability with the highest value\n",
    "        predictions = np.argmax(probabilidades_clase,axis=0)\n",
    "        #We replace the indexes with the actual value of the label\n",
    "        final_preds = pd.Series(predictions).replace({i:label for i,label in enumerate(self.labels)})\n",
    "\n",
    "        return final_preds\n",
    "\n",
    "    def get_modelos(self):\n",
    "        return self.fitted_classifiers\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Fit the model to the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X predictor variables\n",
    "        y predictor variable\n",
    "        \"\"\"\n",
    "        self._fit(X,y)\n",
    "    def predict(self, X) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Makes a prediction on a set of predictor variables\n",
    "        Parameters\n",
    "        ----------\n",
    "        X predictor variables\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        labels labels predicted by the model\n",
    "        \"\"\"\n",
    "        return self._predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Carga de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"rating.csv\", delimiter=\";\", decimal=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.iloc[:, :-1]\n",
    "y = data[\"RATE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, zero_one_loss \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "results = {\n",
    "    \"model\": [],\n",
    "    \"MZE\": [],\n",
    "    \"MAE\": []\n",
    "}\n",
    "\n",
    "# Create instances of the classifiers\n",
    "classifier1 = RandomForestClassifier()\n",
    "classifier2 = SVC(probability=True)  # SVC needs probability estimates\n",
    "classifier3 = MLPClassifier((200, 100, 39))\n",
    "\n",
    "# Create instances of MultipleModelOrdinalClassifier\n",
    "ordinal_classifier1 = MultipleModelOrdinalClassifier(classifier1)\n",
    "ordinal_classifier2 = MultipleModelOrdinalClassifier(classifier2)\n",
    "ordinal_classifier3 = MultipleModelOrdinalClassifier(classifier3)\n",
    "\n",
    "# Compare results without using MultipleModelOrdinalClassifier\n",
    "predictions1 = cross_val_predict(classifier1, X, y, cv=10)\n",
    "mze1 = zero_one_loss(y, predictions1)\n",
    "mae1 = mean_absolute_error(y, predictions1)\n",
    "\n",
    "results['model'].append('RandomForest')\n",
    "results['MZE'].append(mze1)\n",
    "results['MAE'].append(mae1)\n",
    "\n",
    "predictions2 = cross_val_predict(classifier2, X, y, cv=10)\n",
    "mze2 = zero_one_loss(y, predictions2)\n",
    "mae2 = mean_absolute_error(y, predictions2)\n",
    "results['model'].append('SVC')\n",
    "results['MZE'].append(mze2)\n",
    "results['MAE'].append(mae2)\n",
    "\n",
    "predictions3 = cross_val_predict(classifier3, X, y, cv=10)\n",
    "mze3 = zero_one_loss(y, predictions3)\n",
    "mae3 = mean_absolute_error(y, predictions3)\n",
    "results['model'].append('MLP')\n",
    "results['MZE'].append(mze3)\n",
    "results['MAE'].append(mae3)\n",
    "\n",
    "# Compare results using MultipleModelOrdinalClassifier\n",
    "ordinal_predictions1 = cross_val_predict(ordinal_classifier1, X, y, cv=10)\n",
    "ordinal_mze1 = zero_one_loss(y, ordinal_predictions1)\n",
    "ordinal_mae1 = mean_absolute_error(y, ordinal_predictions1)\n",
    "results['model'].append('MMOC - RandomForest')\n",
    "results['MZE'].append(ordinal_mze1)\n",
    "results['MAE'].append(ordinal_mae1)\n",
    "\n",
    "ordinal_predictions2 = cross_val_predict(ordinal_classifier2, X, y, cv=10)\n",
    "ordinal_mze2 = zero_one_loss(y, ordinal_predictions2)\n",
    "ordinal_mae2 = mean_absolute_error(y, ordinal_predictions2)\n",
    "results['model'].append('MMOC - SVC')\n",
    "results['MZE'].append(ordinal_mze2)\n",
    "results['MAE'].append(ordinal_mae2)\n",
    "\n",
    "ordinal_predictions3 = cross_val_predict(ordinal_classifier3, X, y, cv=10)\n",
    "ordinal_mze3 = zero_one_loss(y, ordinal_predictions3)\n",
    "ordinal_mae3 = mean_absolute_error(y, ordinal_predictions3)\n",
    "results['model'].append('MMOC - MLP')\n",
    "results['MZE'].append(ordinal_mze3)\n",
    "results['MAE'].append(ordinal_mae3)\n",
    "\n",
    "df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrr}\n",
      "\\toprule\n",
      "              model &      MZE &      MAE \\\\\n",
      "\\midrule\n",
      "       RandomForest & 0.330502 & 0.363707 \\\\\n",
      "                SVC & 0.558301 & 0.795367 \\\\\n",
      "                MLP & 0.566795 & 0.736680 \\\\\n",
      "MMOC - RandomForest & 0.314286 & 0.335135 \\\\\n",
      "         MMOC - SVC & 0.579923 & 0.641699 \\\\\n",
      "         MMOC - MLP & 0.547490 & 0.681853 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(df.to_latex(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos ver que en los tres modelos usados, el MAE mejora al emplear el enfoque \n",
    "Multiple Ordinal, sin embargo el MZE (1-accuracy) es ligeramente menor en Random \n",
    "Forest y SVC.\n",
    "\n",
    "Esto se debe a que el error disminuye en todos los modelos debido a que las\n",
    "predicciones erroneas se acercan más a la predición real teniendo en cuenta\n",
    "el orden, por lo que se obtiene un valor de MAE menor. Sin embargo, el número de \n",
    "elementos bien clasificados es similar (ligeramente inferior en RF y SVC y superiro\n",
    "en MLP), por lo que no hay grandes variaciones en el MZE.\n",
    "\n",
    "En este caso concreto el resultado es beneficioso debido a que, a pesar de que\n",
    "falle/acierte en un número similar de instancias, el error que comete es menor,\n",
    "es decir, la distancia entre la clase real y la predicha es menor,\n",
    "y no es lo mismo clasificar un crédito bancario con valor 1 cuando realmente es \n",
    "4 que clasificarlo como 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Ejercicio 2\n",
    "### Monotonic Regression with XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'model': [],\n",
    "    'MZE': [],\n",
    "    'MAE': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard XGBoost\n",
    "Aplicamos regresión monotónica usando la versión estándar de XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformamos las clases porque XGBClassifier espera que las clases\n",
    "sean de 0 a n-1 donde n es el número de clases, pero nuestras\n",
    "clases toman valores de 1 a n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_xgb = y - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que se especifica que todos los valores de entrada tienen una relación\n",
    "monótona inversa al valor de la clase, lo especificamos en las restricciones de\n",
    "monotonicidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Monotonicity\n",
    "# 0: without constrains, 1: positive, -1: negative\n",
    "feature_monotones = [-1] * (X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MZE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.318919</td>\n",
       "      <td>0.349035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           model       MZE       MAE\n",
       "0  XGBClassifier  0.318919  0.349035"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Hiperparámetros de XGBoost\n",
    "params = {'max_depth': 2,\n",
    "          'eta': 0.1,\n",
    "          'nthread': 2,\n",
    "          'seed': 0,\n",
    "          'monotone_constraints': '(' + ','.join([str(m) for m in feature_monotones]) + ')'\n",
    "         }\n",
    "\n",
    "clf = XGBClassifier(**params)\n",
    "\n",
    "# Validación cruzada 10-fold\n",
    "predictions_xgb = cross_val_predict(clf, X, y_xgb, cv=10)\n",
    "\n",
    "# Cálculo de las métricas de error\n",
    "xgb_mze = zero_one_loss(y_xgb, predictions_xgb)\n",
    "xgb_mae = mean_absolute_error(y_xgb, predictions_xgb)\n",
    "\n",
    "results['model'].append('XGBClassifier')\n",
    "results['MZE'].append(xgb_mze)\n",
    "results['MAE'].append(xgb_mae)\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monotonic Regression with OVA XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementación de la versión OVA de XGBoost. \n",
    "\n",
    "Se ha usado la implementación \n",
    "proporcionada y se ha adaptado ligeramente para permitir el paso de parámetros al \n",
    "clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementación de la versión OVA de XGBoost\n",
    "class XGBMonotonicClassifier(BaseEstimator,ClassifierMixin): \n",
    "    # I inherit from these classes so that the class is compatible with the sklearn API.\n",
    "    def __init__(self, *, xgb_params=None):\n",
    "        self.xgboosters = []\n",
    "        self.xgb_params = xgb_params \n",
    "\n",
    "    def _fit(self,X,y):\n",
    "        #We keep the labels and put them in order\n",
    "        self.labels = np.sort(y.unique())\n",
    "        #Iterate through all the labels except the last one\n",
    "        for i,label in enumerate(self.labels[:-1]):\n",
    "            smaller_labels = self.labels[:i+1] #We make the set of tags <= than the current one\n",
    "            greater_labels = self.labels[i+1:] #Makes the set of labels > than the current one\n",
    "\n",
    "            #We build two dictionaries to replace the label values in the dataset.\n",
    "            smaller_replacements = { label:0 for label in smaller_labels} \n",
    "            greater_replacements = { label:1 for label in greater_labels} \n",
    "            smaller_replacements.update(greater_replacements)\n",
    "\n",
    "            y_i = y.replace(smaller_replacements)\n",
    "\n",
    "            #We create an xgb classifier and fit it to the dataset on the replaced tags.\n",
    "            #We place monotonic constraints on all variables with monotone_constraints = \"1\"\n",
    "            classifier = XGBClassifier(**self.xgb_params)\n",
    "            classifier.fit(X,y_i)\n",
    "            #We put the classifier in xgboosters\n",
    "            self.xgboosters.append(classifier)\n",
    "\n",
    "    def _predict(self,X):\n",
    "        predictions = np.zeros(X.shape[0]) #We initialise the array of predictions with everything 0s\n",
    "        for xgboost in self.xgboosters:\n",
    "            #For each xgboost we make a prediction that will give 0 or 1\n",
    "            predictions += xgboost.predict(X) #We add up the prediction of the examples which will be 0 or 1\n",
    "            \n",
    "        #Replace the indexes of the labels with the actual labels\n",
    "        final_preds = pd.Series(predictions).replace({i:label for i,label in enumerate(self.labels)})\n",
    "\n",
    "        return final_preds\n",
    "\n",
    "\n",
    "    def fit(self,X, y):\n",
    "        \"\"\"\n",
    "        Fit the model to the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X predictor variables\n",
    "        y predictor variable\n",
    "        \"\"\"\n",
    "        self._fit(X,y)\n",
    "\n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Makes a prediction on a set of predictor variables\n",
    "        Parameters\n",
    "        ----------\n",
    "        X predictor variables\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        labels labels predicted by the model\n",
    "        \"\"\"\n",
    "        return self._predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MZE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>XGBClassifier</td>\n",
       "      <td>0.318919</td>\n",
       "      <td>0.349035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XGBClassifier - OVA</td>\n",
       "      <td>0.366023</td>\n",
       "      <td>0.392278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 model       MZE       MAE\n",
       "0        XGBClassifier  0.318919  0.349035\n",
       "1  XGBClassifier - OVA  0.366023  0.392278"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Hiperparámetros de XGBoost\n",
    "params = {'max_depth': 2,\n",
    "          'eta': 0.1,\n",
    "          'nthread': 2,\n",
    "          'seed': 0,\n",
    "          'monotone_constraints': '(' + ','.join([str(m) for m in feature_monotones]) + ')'\n",
    "         }\n",
    "\n",
    "clf = XGBMonotonicClassifier(xgb_params=params)\n",
    "\n",
    "# Validación cruzada 10-fold\n",
    "predictions_xgb = cross_val_predict(clf, X, y_xgb, cv=10)\n",
    "\n",
    "# Cálculo de las métricas de error\n",
    "xgb_mze = zero_one_loss(y_xgb, predictions_xgb)\n",
    "xgb_mae = mean_absolute_error(y_xgb, predictions_xgb)\n",
    "\n",
    "results['model'].append('XGBClassifier - OVA')\n",
    "results['MZE'].append(xgb_mze)\n",
    "results['MAE'].append(xgb_mae)\n",
    "\n",
    "pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso vemos que el algoritmo XGBoost estándar nos da un mejor \n",
    "resultado que la aproximación One-Versus-All en ambas métricas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monotonic Regression with LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'model': [],\n",
    "    'MZE': [],\n",
    "    'MAE': []\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MZE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.30888</td>\n",
       "      <td>0.337452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      model      MZE       MAE\n",
       "0  LightGBM  0.30888  0.337452"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LGBMClassifier(monotone_constraints=feature_monotones, verbosity=-1)\n",
    "predictions_lgbm = cross_val_predict(clf, X, y, cv=10)\n",
    "\n",
    "lgbm_mze = zero_one_loss(y, predictions_lgbm)\n",
    "lgbm_mae = mean_absolute_error(y, predictions_lgbm)\n",
    "\n",
    "results['model'].append('LightGBM')\n",
    "results['MZE'].append(lgbm_mze)\n",
    "results['MAE'].append(lgbm_mae)\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OVA LightGBM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Implementación de la versión OVA de LightGBM\n",
    "class LightGBMonotonicClassifier(BaseEstimator,ClassifierMixin): \n",
    "    # I inherit from these classes so that the class is compatible with the sklearn API.\n",
    "    def __init__(self, *, lgbm_params=None):\n",
    "        self.lightgbms = []\n",
    "        self.lgbm_params = lgbm_params \n",
    "\n",
    "    def _fit(self,X,y):\n",
    "        #We keep the labels and put them in order\n",
    "        self.labels = np.sort(y.unique())\n",
    "        #Iterate through all the labels except the last one\n",
    "        for i,label in enumerate(self.labels[:-1]):\n",
    "            smaller_labels = self.labels[:i+1] #We make the set of tags <= than the current one\n",
    "            greater_labels = self.labels[i+1:] #Makes the set of labels > than the current one\n",
    "\n",
    "            #We build two dictionaries to replace the label values in the dataset.\n",
    "            smaller_replacements = { label:0 for label in smaller_labels} \n",
    "            greater_replacements = { label:1 for label in greater_labels} \n",
    "            smaller_replacements.update(greater_replacements)\n",
    "\n",
    "            y_i = y.replace(smaller_replacements)\n",
    "\n",
    "            #We create an lgbm classifier and fit it to the dataset on the replaced tags.\n",
    "            #We place monotonic constraints on all variables with monotone_constraints = \"1\"\n",
    "            classifier = LGBMClassifier(**self.lgbm_params)\n",
    "            classifier.fit(X,y_i)\n",
    "            #We put the classifier in xgboosters\n",
    "            self.lightgbms.append(classifier)\n",
    "\n",
    "    def _predict(self,X):\n",
    "        predictions = np.zeros(X.shape[0]) #We initialise the array of predictions with everything 0s\n",
    "        for lgbm in self.lightgbms:\n",
    "            #For each lightgbm we make a prediction that will give 0 or 1\n",
    "            predictions += lgbm.predict(X) #We add up the prediction of the examples which will be 0 or 1\n",
    "            \n",
    "        #Replace the indexes of the labels with the actual labels\n",
    "        final_preds = pd.Series(predictions).replace({i:label for i,label in enumerate(self.labels)})\n",
    "\n",
    "        return final_preds\n",
    "\n",
    "\n",
    "    def fit(self,X, y):\n",
    "        \"\"\"\n",
    "        Fit the model to the data\n",
    "        Parameters\n",
    "        ----------\n",
    "        X predictor variables\n",
    "        y predictor variable\n",
    "        \"\"\"\n",
    "        self._fit(X,y)\n",
    "\n",
    "    def predict(self,X):\n",
    "        \"\"\"\n",
    "        Makes a prediction on a set of predictor variables\n",
    "        Parameters\n",
    "        ----------\n",
    "        X predictor variables\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        labels labels predicted by the model\n",
    "        \"\"\"\n",
    "        return self._predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>MZE</th>\n",
       "      <th>MAE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBM</td>\n",
       "      <td>0.308880</td>\n",
       "      <td>0.337452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>LightGBM - OVA</td>\n",
       "      <td>0.272587</td>\n",
       "      <td>0.288803</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            model       MZE       MAE\n",
       "0        LightGBM  0.308880  0.337452\n",
       "1  LightGBM - OVA  0.272587  0.288803"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgbm_params = {\n",
    "    'monotene_constraints': feature_monotones,\n",
    "    'verbosity': -1\n",
    "}\n",
    "\n",
    "clf = LightGBMonotonicClassifier(lgbm_params=lgbm_params)\n",
    "predictions_lgbm = cross_val_predict(clf, X, y, cv=10)\n",
    "\n",
    "lgbm_mze = zero_one_loss(y, predictions_lgbm)\n",
    "lgbm_mae = mean_absolute_error(y, predictions_lgbm)\n",
    "\n",
    "results['model'].append('LightGBM - OVA')\n",
    "results['MZE'].append(lgbm_mze)\n",
    "results['MAE'].append(lgbm_mae)\n",
    "\n",
    "pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, al contrario que con XGBoost vemos una mejora significativa\n",
    "de la versión One-Versus-All frente a la estándar aplicando las mismas\n",
    "restricciones de monotonía en ambas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
